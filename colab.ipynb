import pdfplumber
import spacy
import pandas as pd
from google.colab import files
from spacy.lang.en.stop_words import STOP_WORDS

# Load the spacy model
nlp = spacy.load('en_core_web_sm')

# Function to extract and clean sentences from a PDF
def extract_and_clean_sentences_from_pdf(pdf_path):
    """
    Extract and clean sentences from a PDF:
    - Combine lines until a full stop is encountered.
    - Remove sentences shorter than 5 characters.
    """
    sentences = []
    with pdfplumber.open(pdf_path) as pdf:
        for page in pdf.pages:
            text = page.extract_text()
            if text:
                page_sentences = nlp(text).sents
                temp_sentence = ""
                for sent in page_sentences:
                    temp_sentence += " " + sent.text.strip()
                    if sent.text.strip().endswith('.'):  # End of sentence
                        if len(temp_sentence.strip()) >= 5:  # Filter short sentences
                            sentences.append(temp_sentence.strip())
                        temp_sentence = ""  # Reset for the next sentence
    return sentences

def highlight_most_important_noun(sentence):
    """
    Use spacy for POS tagging to identify the most important noun and replace it with a blank.
    The noun is chosen based on its syntactic importance (based on dependency parsing).
    Avoid stop words as answers.
    """
    doc = nlp(sentence)
    most_important_noun = None
    max_importance = -1
    for token in doc:
        if token.pos_ == 'NOUN' and token.text.lower() not in STOP_WORDS:
            importance = token.dep_
            if importance in ['nsubj', 'dobj', 'attr']:
                importance_value = 2
            else:
                importance_value = 1

            if importance_value > max_importance:
                max_importance = importance_value
                most_important_noun = token.text

    if not most_important_noun:
        for token in doc:
            if token.pos_ == 'NOUN' and token.text.lower() not in STOP_WORDS:
                most_important_noun = token.text
                break

    if most_important_noun:
        highlighted_sentence = sentence.replace(most_important_noun, '_')
    else:
        highlighted_sentence = sentence

    return highlighted_sentence, most_important_noun

def generate_questions(sentence, key_noun):
    """
    Generate H-form and W-form questions based on the sentence and key noun.
    """
    doc = nlp(sentence)
    questions = []

    if key_noun:
        questions.append(f"What is {key_noun}?")
        questions.append(f"Why is {key_noun} important?")
        questions.append(f"Where is {key_noun} located?")

    # Generate Who/How/When based on verbs or named entities
    for token in doc:
        if token.pos_ == 'VERB':
            questions.append(f"How does {token.text} relate to {key_noun}?")
        if token.ent_type_ == 'PERSON':
            questions.append(f"Who is mentioned in relation to {key_noun}?")
        if token.ent_type_ in ['DATE', 'TIME']:
            questions.append(f"When does {token.text} occur?")

    return questions

# Upload the PDF file in Google Colab
uploaded = files.upload()

# Get the PDF file name after upload
pdf_file = next(iter(uploaded))

# Extract and clean sentences from the uploaded PDF
sentences = extract_and_clean_sentences_from_pdf(pdf_file)

highlighted_sentences = []
question_list = []

for sentence in sentences:
    highlighted, key_noun = highlight_most_important_noun(sentence)
    questions = generate_questions(sentence, key_noun)

    highlighted_sentences.append({
        "Sentence": sentence,
        "Fill-in-the-Blank": highlighted,
        "Key Noun": key_noun
    })

    question_list.extend([{
        "Original Sentence": sentence,
        "Question": question
    } for question in questions])

# Create DataFrames to store the results
df_highlighted = pd.DataFrame(highlighted_sentences)
df_questions = pd.DataFrame(question_list)

# Save the DataFrames to Excel files
highlighted_file = "highlighted_sentences.xlsx"
questions_file = "generated_questions.xlsx"

df_highlighted.to_excel(highlighted_file, index=False)
df_questions.to_excel(questions_file, index=False)

# Download the Excel files in Colab
files.download(highlighted_file)
files.download(questions_file)

print("Processing complete. Highlighted sentences and questions saved.")